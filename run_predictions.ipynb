{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numpy import array, NaN\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=pd.read_csv(\"final_evaluation_data.csv\")\n",
    "\n",
    "df_data={\n",
    "    'pair_id':[],\n",
    "    'title1':[],\n",
    "    'text1':[],\n",
    "    'title2':[],\n",
    "    'text2':[],\n",
    "    'Overall':[],\n",
    "    'Language1':[],\n",
    "    'Language2':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4902it [06:09, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, i in tqdm(enumerate(directory['pair_id'])):\n",
    "    id1, id2 = i.split('_')\n",
    "    try:\n",
    "        file1=json.load(open(f\"eval_data\\eval_data\\{str(id1)[-2:]}\\{id1}.json\"))\n",
    "        file2=json.load(open(f\"eval_data\\eval_data\\{str(id2)[-2:]}\\{id2}.json\"))\n",
    "        df_data['pair_id'].append(i)\n",
    "        \n",
    "        if file1['title'].strip() == '':\n",
    "            df_data['title1'].append(NaN)\n",
    "        else:\n",
    "            df_data['title1'].append(file1['title'])\n",
    "\n",
    "        if file1['text'].strip() == '':\n",
    "            df_data['text1'].append(NaN)\n",
    "        else:\n",
    "            df_data['text1'].append(file1['text'])\n",
    "\n",
    "        if str(file2['title'].strip()) == '':\n",
    "            df_data['title2'].append(NaN)\n",
    "        else:\n",
    "            df_data['title2'].append(file2['title'])\n",
    "\n",
    "        if file2['text'].strip() == '':\n",
    "            df_data['text2'].append(NaN)\n",
    "        else:\n",
    "            df_data['text2'].append(file2['text'])\n",
    "        \n",
    "        df_data['Overall'].append(directory['Overall'][idx])\n",
    "        df_data['Language1'].append(directory['url1_lang'][idx])\n",
    "        df_data['Language2'].append(directory['url2_lang'][idx])\n",
    "        \n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(df_data)\n",
    "df.dropna(inplace=True)\n",
    "df=df.reset_index()\n",
    "df=df.drop(['index'], axis=1)\n",
    "df.to_csv('test_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_eng = set(stopwords.words('english'))\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "stop_words_tr = set(stopwords.words('turkish'))\n",
    "stop_words_de = set(stopwords.words('german'))\n",
    "stop_words_ar = set(stopwords.words('arabic'))\n",
    "stop_words_it = set(stopwords.words('italian'))\n",
    "stop_words_ru = set(stopwords.words('russian'))\n",
    "\n",
    "stopwords={\n",
    "    'en':stop_words_eng,\n",
    "    'fr':stop_words_fr,\n",
    "    'es':stop_words_es,\n",
    "    'tr':stop_words_tr,\n",
    "    'ar':stop_words_ar,\n",
    "    'de':stop_words_de,\n",
    "    'it':stop_words_it,\n",
    "    'ru':stop_words_ru\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4693 [00:00<?, ?it/s]<ipython-input-55-2d7f4f9aae8a>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text1'][i] = ' '.join([w.lower() for w in word_tokens_title1.split() if not w.lower() in stopwords[df['Language1'][i]]])\n",
      "<ipython-input-55-2d7f4f9aae8a>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text1'][i] +=' '+' '.join([w.lower() for w in word_tokens_text1.split() if not w.lower() in stopwords[df['Language1'][i]]])\n",
      "<ipython-input-55-2d7f4f9aae8a>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text2'][i] = ' '.join([w.lower() for w in word_tokens_title2.split() if not w.lower() in stopwords[df['Language2'][i]]])\n",
      "<ipython-input-55-2d7f4f9aae8a>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text2'][i] +=' '+' '.join([w.lower() for w in word_tokens_text2.split() if not w.lower() in stopwords[df['Language2'][i]]])\n",
      "100%|██████████| 4693/4693 [00:15<00:00, 304.41it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(df))):\n",
    "    if df['Language1'][i]=='pl' or df['Language2'][i]=='pl' or df['Language1'][i]=='zh' or df['Language2'][i]=='zh':\n",
    "        continue\n",
    "    word_tokens_title1 = df['title1'][i]\n",
    "    word_tokens_text1 = df['text1'][i]\n",
    "    word_tokens_title2 = df['title2'][i]\n",
    "    word_tokens_text2 = df['text2'][i]\n",
    "\n",
    "    df['text1'][i] = ' '.join([w.lower() for w in word_tokens_title1.split() if not w.lower() in stopwords[df['Language1'][i]]])\n",
    "    df['text1'][i] +=' '+' '.join([w.lower() for w in word_tokens_text1.split() if not w.lower() in stopwords[df['Language1'][i]]])\n",
    "    df['text2'][i] = ' '.join([w.lower() for w in word_tokens_title2.split() if not w.lower() in stopwords[df['Language2'][i]]])\n",
    "    df['text2'][i] +=' '+' '.join([w.lower() for w in word_tokens_text2.split() if not w.lower() in stopwords[df['Language2'][i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 0', 'title1', 'title2'], axis=1)\n",
    "df=df.rename(columns={\n",
    "    'text1': 'article1',\n",
    "    'text2': 'article2'\n",
    "})\n",
    "df.to_csv('preprocessed_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('preprocessed.csv')\n",
    "train_data = train_data.dropna()\n",
    "train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('preprocessed_test.csv')\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_test, y_train = [], [], [], [] # Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(train_data)):\n",
    "  X_train.append(train_data['article1'][idx])\n",
    "  X_train.append(train_data['article2'][idx])\n",
    "  y_train.append(train_data['Overall'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_data)):\n",
    "  X_test.append(test_data['article1'][idx])\n",
    "  X_test.append(test_data['article2'][idx])\n",
    "  y_test.append(test_data['Overall'][idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(min_df =4) # Term frequency - inverse document frequency\n",
    "x_train = tf_vec.fit(X_train)\n",
    "x_test = tf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_art1 = []\n",
    "X_test_art2 = []\n",
    "X_test1 = []\n",
    "X_test2 = []\n",
    "\n",
    "for j in range(len(test_data)):\n",
    "  art1 = 2*idx\n",
    "  art2 = 2*idx+1\n",
    "  X_test_art1.append(x_test[art1])\n",
    "  X_test_art2.append(x_test[art2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "  X_test1.append(array(csr_matrix.todense(X_test_art1[i])))\n",
    "  X_test2.append(array(csr_matrix.todense(X_test_art2[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = array(X_test1)\n",
    "X_test2 = array(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = X_test1.reshape(len(test_data),X_test1.shape[2])\n",
    "X_test2 = X_test2.reshape(len(test_data),X_test2.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"Saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test1, X_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0620849041219411"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(predictions, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c164560968b914d16428dcacec828cdca4d315bcec8e60a94d52933a7d476de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
